{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE `annadl_f19` folder** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handin in Peergrade**: *Friday*, November 15, 2019, 23:59<br>\n",
    "**Peergrading deadline**: *Monday*, November 18, 2019, 23:59<br>\n",
    "**Peergrading feedback deadline**: *Friday*, November 22, 2019, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Feedback**](http://ulfaslak.com/vent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.4**: Did the network finish training? Consider the generated text across epochs.\n",
    "1. In the early batches (0-10), the generated text looks very bad. Can you explain why the low diversity generated text contains almost only the symbol \" \" (that is, spaces)?\n",
    "2. The high diversity generated text is messed up too, but in a different way. Explain how.\n",
    "3. In later batches (20-30) what do you notice is off about the low diversity generated text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.6**: Do the same as above, but for 40 random letters (e.g. smash away on your keyboard) as seed. What happens? Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.1**: In your own words, explain what the following function arguments do in\n",
    "the different model loading functions:\n",
    "1. `include_top`\n",
    "1. `weights`\n",
    "1. `input_shape`\n",
    "1. `pooling`\n",
    "1. `classes`\n",
    "1. Explain what 'global pooling' does, and why it is needed when `include_top=False`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.2**: Following Jason's example under 'Pre-Trained Model as Classifier'\n",
    "classify [this image](https://66.media.tumblr.com/tumblr_mc46e7Zm4R1qbqngeo1_1280.jpg).\n",
    "Print not just the most likely label, but everything that `decode_predictions` returns.\n",
    ">\n",
    "> ***Important***: *Don't use VGG as he does. It's 500 MB to download, and will take too long.\n",
    "> Use one of the smaller models instead ([here](https://keras.io/applications/#documentation-for-individual-models)'s an overview of model sizes), such as DenseNet121.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.1**: What is typically the input and output of an autoencoder? What loss function can be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.3**: Purely in terms of architecture, what is the difference between an autoencoder and a variational autoencoder (VAE)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.4**: Regular autoencoders are trained to minimize a loss function with no regard to how the latent space is organized. Therefore, continuity is not guaranteed and similar datapoints may not be close to each other. We can thus say that the network is overfitting, because it uses any organization of training points in this space to minimize the loss, and is, therefore, not likely to work well with unseen data. VAEs are a regularized form of autoencoders, invented to solve this problem. Importantly, they guarantee that similar points are close in the latent space. How do they achieve this?\n",
    "    > * How are datapoints represented in the VAE latent space? What is the intuition behind this?\n",
    "    > * How is the loss function different? What is the purpose of the second term (the KL divergence)?\n",
    ">\n",
    "> *Hint: Check out this [blog post](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73) and read the section \"Intuitions about the regularisation\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.1**: Explain in your own words how the GAN works. Touch upon:\n",
    "    > * What do the generator and discriminator networks do?\n",
    "    > * What are their respective input and output?\n",
    "    > * What would the accuracy of the discriminator be, faced with a perfect generator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.4**: How do you transform one image to another using backprop and\n",
    "gradient descent? Why does this not always work that well? How is transfer learning\n",
    "used to make it work?"
   ]
  }
 ],
 "metadata": {
  "display_name": "Python 3",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
