{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE annadl_f19 folder**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: Generative models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond neural networks for classifying and predicting, there exists an enormous zoo\n",
    "of models for doing \"other things\". One very interesting class of models are called\n",
    "\"generative models\" and as the name suggests, are used for *generating* data. What\n",
    "they do, is learn the underlying structure of the data, such that one can query this\n",
    "structure and obtain new datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're steadily moving into \"project territory\". Therefore, I feel it's more conductive that you build an intuition for how two pillars of generative models (VAEs and GANs) work, rather than work through code. For your understanding's sake, I will ask some questions about the videos you watched for this week. Try to answer these questions as if you were explaining their answer to someone else (like yourself sometime in the future). Make sure your explanations are clear and that you fully understand your own answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have watched [this video](https://www.youtube.com/watch?v=9zKuYvjFFS8), please answer the questions below. I also throw in some questions that link to other sources, to prompt you for a deeper understanding of some of the intuition behind VAEs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.1**: What is typically the input and output of an autoencoder? What loss function can be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.2**: What is the \"bottleneck\" of an autoencoder? What can it be used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.3**: Purely in terms of architecture, what is the difference between an autoencoder and a variational autoencoder (VAE)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.4**: Regular autoencoders are trained to minimize a loss function with no regard to how the latent space is organized. Therefore, continuity is not guaranteed and similar datapoints may not be close to each other. We can thus say that the network is overfitting, because it uses any organization of training points in this space to minimize the loss, and is, therefore, not likely to work well with unseen data. VAEs are a regularized form of autoencoders, invented to solve this problem. Importantly, they guarantee that similar points are close in the latent space. How do they achieve this?\n",
    "    > * How are datapoints represented in the VAE latent space? What is the intuition behind this?\n",
    "    > * How is the loss function different? What is the purpose of the second term (the KL divergence)?\n",
    ">\n",
    "> *Hint: Check out this [blog post](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73) and read the section \"Intuitions about the regularisation\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.5**: How is the latent vector sampled from the mean and standard deviation vectors? Explain the \"reparameterization trick\" and why it is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.6**: What is the motivation behind the disentangled VAE (or *$\\beta$-VAE*)?\n",
    "What happens is $\\beta$ is too high? What happens when it is too small?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.EXTRA**: If you are curious about why such radical generalization\n",
    "performance increases can be achieved by just including a single new hyperparameter\n",
    "in the cost function, check out [the original paper](https://openreview.net/references/pdf?id=Sy2fzU9gl)\n",
    "from Google Deep Mind. In it, under \"$\\beta$-VAE FRAMEWORK DERIVATION\" you will\n",
    "find the intuition behind this small but powerful design modification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.7**: Give some examples of what autoencoders can be used for. Creativity allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have watched [this video](https://www.youtube.com/watch?v=dCKbRCUyop8), please answer the questions below. I also throw in some questions that link to other sources, to prompt you for a deeper understanding of some of the intuition behind GANs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.1**: Explain in your own words how the GAN works. Touch upon:\n",
    "    > * What do the generator and discriminator networks do?\n",
    "    > * What are their respective input and output?\n",
    "    > * What would the accuracy of the discriminator be, faced with a perfect generator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.2**: What is \"progressive growing\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.3**: In StyleGAN, what is the purpose of the mapping network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.4**: How do you transform one image to another using backprop and\n",
    "gradient descent? Why does this not always work that well? How is transfer learning\n",
    "used to make it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.5**: From [19:20](https://www.youtube.com/watch?v=dCKbRCUyop8&feature=youtu.be&t=1160),\n",
    "outline in bullets the pipeline for obtaining the latent vector for a query image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.6**: So why go through all this trouble just to find, basically, the\n",
    "point in the latent space that represents a given image? This gets explained at\n",
    "[22:39](https://youtu.be/dCKbRCUyop8?t=1359). Summarize the idea and utility of\n",
    "labeling the points in the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.7**: Besides modeling faces, can you give some examples of what GANs can be used for?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
